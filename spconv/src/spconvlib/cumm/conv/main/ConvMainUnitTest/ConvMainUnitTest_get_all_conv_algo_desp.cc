#include <spconvlib/cumm/conv/main/ConvMainUnitTest.h>
namespace spconvlib {
namespace cumm {
namespace conv {
namespace main {
using TensorView = spconvlib::cumm::common::TensorView;
using GemmBasic = spconvlib::cumm::common::GemmBasic;
using GemmBasicHost = spconvlib::cumm::common::GemmBasicHost;
using ConvNVRTCParams = spconvlib::cumm::conv::kernel::ConvNVRTCParams;
using CummNVRTCLib = spconvlib::cumm::common::CummNVRTCLib;
std::vector<tv::gemm::ConvAlgoDesp> ConvMainUnitTest::get_all_conv_algo_desp()   {
  
  std::vector<tv::gemm::ConvAlgoDesp> desps;
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 0;
  desp.dtype_b = 0;
  desp.dtype_c = 0;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 128, 16};
  desp.warp_tile_shape = {32, 32, 8};
  desp.tensorop = {-1, -1, -1};
  desp.num_stage = 2;
  desp.algo = "Simt";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(0, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Simt_f32f32f32f32f32tnt_m32n128k16m32n32k8A1_200_C301LLL_SK", "error", desp.__repr__(), "Simt_f32f32f32f32f32tnt_m32n128k16m32n32k8A1_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 128, 16};
  desp.warp_tile_shape = {32, 32, 8};
  desp.tensorop = {-1, -1, -1};
  desp.num_stage = 2;
  desp.algo = "Simt";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(0, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Simt_f16f16f16f32f32tnt_m32n128k16m32n32k8A1_200_C301LLL_SK", "error", desp.__repr__(), "Simt_f16f16f16f32f32tnt_m32n128k16m32n32k8A1_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 0;
  desp.dtype_b = 0;
  desp.dtype_c = 0;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 256, 8};
  desp.warp_tile_shape = {32, 64, 8};
  desp.tensorop = {-1, -1, -1};
  desp.num_stage = 2;
  desp.algo = "Simt";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(0, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Simt_f32f32f32f32f32tnt_m32n256k8m32n64k8A1_200_C301LLL_SK", "error", desp.__repr__(), "Simt_f32f32f32f32f32tnt_m32n256k8m32n64k8A1_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 256, 8};
  desp.warp_tile_shape = {32, 64, 8};
  desp.tensorop = {-1, -1, -1};
  desp.num_stage = 2;
  desp.algo = "Simt";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(0, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Simt_f16f16f16f32f32tnt_m32n256k8m32n64k8A1_200_C301LLL_SK", "error", desp.__repr__(), "Simt_f16f16f16f32f32tnt_m32n256k8m32n64k8A1_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 0;
  desp.dtype_b = 0;
  desp.dtype_c = 0;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 64, 16};
  desp.warp_tile_shape = {32, 32, 8};
  desp.tensorop = {-1, -1, -1};
  desp.num_stage = 2;
  desp.algo = "Simt";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(0, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Simt_f32f32f32f32f32tnt_m32n64k16m32n32k8A1_200_C301LLL_SK", "error", desp.__repr__(), "Simt_f32f32f32f32f32tnt_m32n64k16m32n32k8A1_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 64, 16};
  desp.warp_tile_shape = {32, 32, 8};
  desp.tensorop = {-1, -1, -1};
  desp.num_stage = 2;
  desp.algo = "Simt";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(0, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Simt_f16f16f16f32f32tnt_m32n64k16m32n32k8A1_200_C301LLL_SK", "error", desp.__repr__(), "Simt_f16f16f16f32f32tnt_m32n64k16m32n32k8A1_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 0;
  desp.dtype_b = 0;
  desp.dtype_c = 0;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {32, 32, 8};
  desp.tensorop = {-1, -1, -1};
  desp.num_stage = 2;
  desp.algo = "Simt";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(0, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Simt_f32f32f32f32f32tnt_m32n32k32m32n32k8A1_200_C301LLL_SK", "error", desp.__repr__(), "Simt_f32f32f32f32f32tnt_m32n32k32m32n32k8A1_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {32, 32, 8};
  desp.tensorop = {-1, -1, -1};
  desp.num_stage = 2;
  desp.algo = "Simt";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(0, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Simt_f16f16f16f32f32tnt_m32n32k32m32n32k8A1_200_C301LLL_SK", "error", desp.__repr__(), "Simt_f16f16f16f32f32tnt_m32n32k32m32n32k8A1_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 0;
  desp.dtype_b = 0;
  desp.dtype_c = 0;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 256, 8};
  desp.warp_tile_shape = {32, 64, 8};
  desp.tensorop = {-1, -1, -1};
  desp.num_stage = 2;
  desp.algo = "Simt";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(0, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Simt_f32f32f32f32f32tnt_m64n256k8m32n64k8A1_200_C301LLL_SK", "error", desp.__repr__(), "Simt_f32f32f32f32f32tnt_m64n256k8m32n64k8A1_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 256, 8};
  desp.warp_tile_shape = {32, 64, 8};
  desp.tensorop = {-1, -1, -1};
  desp.num_stage = 2;
  desp.algo = "Simt";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(0, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Simt_f16f16f16f32f32tnt_m64n256k8m32n64k8A1_200_C301LLL_SK", "error", desp.__repr__(), "Simt_f16f16f16f32f32tnt_m64n256k8m32n64k8A1_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 0;
  desp.dtype_b = 0;
  desp.dtype_c = 0;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 8};
  desp.warp_tile_shape = {32, 64, 8};
  desp.tensorop = {-1, -1, -1};
  desp.num_stage = 2;
  desp.algo = "Simt";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(0, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Simt_f32f32f32f32f32tnt_m64n128k8m32n64k8A1_200_C301LLL_SK", "error", desp.__repr__(), "Simt_f32f32f32f32f32tnt_m64n128k8m32n64k8A1_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 8};
  desp.warp_tile_shape = {32, 64, 8};
  desp.tensorop = {-1, -1, -1};
  desp.num_stage = 2;
  desp.algo = "Simt";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(0, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Simt_f16f16f16f32f32tnt_m64n128k8m32n64k8A1_200_C301LLL_SK", "error", desp.__repr__(), "Simt_f16f16f16f32f32tnt_m64n128k8m32n64k8A1_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 0;
  desp.dtype_b = 0;
  desp.dtype_c = 0;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 8};
  desp.warp_tile_shape = {32, 32, 8};
  desp.tensorop = {-1, -1, -1};
  desp.num_stage = 2;
  desp.algo = "Simt";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(0, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Simt_f32f32f32f32f32tnt_m64n64k8m32n32k8A1_200_C301LLL_SK", "error", desp.__repr__(), "Simt_f32f32f32f32f32tnt_m64n64k8m32n32k8A1_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 8};
  desp.warp_tile_shape = {32, 32, 8};
  desp.tensorop = {-1, -1, -1};
  desp.num_stage = 2;
  desp.algo = "Simt";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(0, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Simt_f16f16f16f32f32tnt_m64n64k8m32n32k8A1_200_C301LLL_SK", "error", desp.__repr__(), "Simt_f16f16f16f32f32tnt_m64n64k8m32n32k8A1_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 0;
  desp.dtype_b = 0;
  desp.dtype_c = 0;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 32, 16};
  desp.warp_tile_shape = {32, 32, 8};
  desp.tensorop = {-1, -1, -1};
  desp.num_stage = 2;
  desp.algo = "Simt";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(0, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Simt_f32f32f32f32f32tnt_m64n32k16m32n32k8A1_200_C301LLL_SK", "error", desp.__repr__(), "Simt_f32f32f32f32f32tnt_m64n32k16m32n32k8A1_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 32, 16};
  desp.warp_tile_shape = {32, 32, 8};
  desp.tensorop = {-1, -1, -1};
  desp.num_stage = 2;
  desp.algo = "Simt";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(0, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Simt_f16f16f16f32f32tnt_m64n32k16m32n32k8A1_200_C301LLL_SK", "error", desp.__repr__(), "Simt_f16f16f16f32f32tnt_m64n32k16m32n32k8A1_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 4};
  desp.num_stage = 2;
  desp.algo = "Volta";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Volta_f16f16f16f16f16tnt_m64n64k32m32n32k32A1T884_200_C301LLL_SK", "error", desp.__repr__(), "Volta_f16f16f16f16f16tnt_m64n64k32m32n32k32A1T884_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 4};
  desp.num_stage = 2;
  desp.algo = "Volta";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Volta_f16f16f16f32f32tnt_m64n64k32m32n32k32A1T884_200_C301LLL_SK", "error", desp.__repr__(), "Volta_f16f16f16f32f32tnt_m64n64k32m32n32k32A1T884_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 4};
  desp.num_stage = 2;
  desp.algo = "Volta";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 0;
  desp.min_arch = std::make_tuple(7, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Volta_f16f16f16f16f16tnt_m64n64k32m32n32k32A0T884_200_C301LLL_SK", "error", desp.__repr__(), "Volta_f16f16f16f16f16tnt_m64n64k32m32n32k32A0T884_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 4};
  desp.num_stage = 2;
  desp.algo = "Volta";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 0;
  desp.min_arch = std::make_tuple(7, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Volta_f16f16f16f32f32tnt_m64n64k32m32n32k32A0T884_200_C301LLL_SK", "error", desp.__repr__(), "Volta_f16f16f16f32f32tnt_m64n64k32m32n32k32A0T884_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {8, 8, 4};
  desp.num_stage = 2;
  desp.algo = "Volta";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Volta_f16f16f16f16f16tnt_m64n128k32m32n64k32A1T884_200_C301LLL_SK", "error", desp.__repr__(), "Volta_f16f16f16f16f16tnt_m64n128k32m32n64k32A1T884_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {8, 8, 4};
  desp.num_stage = 2;
  desp.algo = "Volta";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Volta_f16f16f16f32f32tnt_m64n128k32m32n64k32A1T884_200_C301LLL_SK", "error", desp.__repr__(), "Volta_f16f16f16f32f32tnt_m64n128k32m32n64k32A1T884_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 256, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {8, 8, 4};
  desp.num_stage = 2;
  desp.algo = "Volta";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Volta_f16f16f16f16f16tnt_m32n256k32m32n64k32A1T884_200_C301LLL_SK", "error", desp.__repr__(), "Volta_f16f16f16f16f16tnt_m32n256k32m32n64k32A1T884_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 256, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {8, 8, 4};
  desp.num_stage = 2;
  desp.algo = "Volta";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Volta_f16f16f16f32f32tnt_m32n256k32m32n64k32A1T884_200_C301LLL_SK", "error", desp.__repr__(), "Volta_f16f16f16f32f32tnt_m32n256k32m32n64k32A1T884_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 0;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8f32s32f32tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8f32s32f32tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 0;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8f32s32f32tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8f32s32f32tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 0;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8f32s32f16tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8f32s32f16tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 0;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8f32s32f16tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8f32s32f16tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 7;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8f16s32f32tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8f16s32f32tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 7;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8f16s32f32tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8f16s32f32tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 7;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8f16s32f16tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8f16s32f16tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 7;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8f16s32f16tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8f16s32f16tnt_m64n64k32m32n32k32A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 0;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 0;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 0;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 0;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 0;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 0;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8f32s32f32tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8f32s32f32tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 0;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 0;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8f32s32f32tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8f32s32f32tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 0;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 0;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8f32s32f16tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8f32s32f16tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 0;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 0;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8f32s32f16tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8f32s32f16tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 7;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 0;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8f16s32f32tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8f16s32f32tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 7;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 0;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8f16s32f32tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8f16s32f32tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 7;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 0;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8f16s32f16tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8f16s32f16tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 7;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 0;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8f16s32f16tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8f16s32f16tnt_m64n64k32m32n32k32A0T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 16, 16};
  desp.warp_tile_shape = {16, 16, 16};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 0;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f16f16tnt_m32n16k16m16n16k16A0T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f16f16tnt_m32n16k16m16n16k16A0T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 16, 16};
  desp.warp_tile_shape = {16, 16, 16};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 0;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f32f32tnt_m32n16k16m16n16k16A0T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f32f32tnt_m32n16k16m16n16k16A0T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 16, 16};
  desp.warp_tile_shape = {16, 16, 16};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f16f16tnt_m32n16k16m16n16k16A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f16f16tnt_m32n16k16m16n16k16A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 16, 16};
  desp.warp_tile_shape = {16, 16, 16};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f32f32tnt_m32n16k16m16n16k16A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f32f32tnt_m32n16k16m16n16k16A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 32, 16};
  desp.warp_tile_shape = {32, 16, 16};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 0;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f16f16tnt_m64n32k16m32n16k16A0T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f16f16tnt_m64n32k16m32n16k16A0T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 32, 16};
  desp.warp_tile_shape = {32, 16, 16};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 0;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f32f32tnt_m64n32k16m32n16k16A0T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f32f32tnt_m64n32k16m32n16k16A0T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 32, 16};
  desp.warp_tile_shape = {32, 16, 16};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f16f16tnt_m64n32k16m32n16k16A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f16f16tnt_m64n32k16m32n16k16A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 32, 16};
  desp.warp_tile_shape = {32, 16, 16};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f32f32tnt_m64n32k16m32n16k16A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f32f32tnt_m64n32k16m32n16k16A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 64, 32};
  desp.warp_tile_shape = {32, 32, 16};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 0;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f16f16tnt_m32n64k32m32n32k16A0T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f16f16tnt_m32n64k32m32n32k16A0T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 64, 32};
  desp.warp_tile_shape = {32, 32, 16};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 0;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f32f32tnt_m32n64k32m32n32k16A0T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f32f32tnt_m32n64k32m32n32k16A0T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 64, 32};
  desp.warp_tile_shape = {32, 32, 16};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f16f16tnt_m32n64k32m32n32k16A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f16f16tnt_m32n64k32m32n32k16A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 64, 32};
  desp.warp_tile_shape = {32, 32, 16};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f32f32tnt_m32n64k32m32n32k16A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f32f32tnt_m32n64k32m32n32k16A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 256, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f16f16tnt_m32n256k32m32n64k32A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f16f16tnt_m32n256k32m32n64k32A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 256, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f32f32tnt_m32n256k32m32n64k32A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f32f32tnt_m32n256k32m32n64k32A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 128, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f16f16tnt_m32n128k32m32n32k32A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f16f16tnt_m32n128k32m32n32k32A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 128, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f32f32tnt_m32n128k32m32n32k32A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f32f32tnt_m32n128k32m32n32k32A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 128, 64};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f16f16tnt_m32n128k64m32n32k32A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f16f16tnt_m32n128k64m32n32k32A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 128, 64};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f32f32tnt_m32n128k64m32n32k32A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f32f32tnt_m32n128k64m32n32k32A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 128, 64};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f16f16tnt_m32n128k64m32n64k32A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f16f16tnt_m32n128k64m32n64k32A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 128, 64};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f32f32tnt_m32n128k64m32n64k32A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f32f32tnt_m32n128k64m32n64k32A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 128, 64};
  desp.warp_tile_shape = {32, 32, 64};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f16f16tnt_m32n128k64m32n32k64A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f16f16tnt_m32n128k64m32n32k64A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 128, 64};
  desp.warp_tile_shape = {32, 32, 64};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f32f32tnt_m32n128k64m32n32k64A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f32f32tnt_m32n128k64m32n32k64A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 128, 64};
  desp.warp_tile_shape = {32, 64, 64};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f16f16tnt_m32n128k64m32n64k64A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f16f16tnt_m32n128k64m32n64k64A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 128, 64};
  desp.warp_tile_shape = {32, 64, 64};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f32f32tnt_m32n128k64m32n64k64A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f32f32tnt_m32n128k64m32n64k64A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f16f16tnt_m64n128k32m32n64k32A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f16f16tnt_m64n128k32m32n64k32A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f32f32tnt_m64n128k32m32n64k32A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f32f32tnt_m64n128k32m32n64k32A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 64};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f16f16tnt_m64n128k64m32n64k32A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f16f16tnt_m64n128k64m32n64k32A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 64};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f32f32tnt_m64n128k64m32n64k32A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f32f32tnt_m64n128k64m32n64k32A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f16f16tnt_m64n64k32m32n32k32A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f16f16tnt_m64n64k32m32n32k32A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_f16f16f16f32f32tnt_m64n64k32m32n32k32A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Turing_f16f16f16f32f32tnt_m64n64k32m32n32k32A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m64n32k32m32n32k32A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m64n32k32m32n32k32A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m64n32k32m32n32k32A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m64n32k32m32n32k32A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m64n32k32m32n32k32A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m64n32k32m32n32k32A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m64n32k32m32n32k32A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m64n32k32m32n32k32A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m32n64k32m32n32k32A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m32n64k32m32n32k32A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m32n64k32m32n32k32A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m32n64k32m32n32k32A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m32n64k32m32n32k32A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m32n64k32m32n32k32A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m32n64k32m32n32k32A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m32n64k32m32n32k32A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m32n32k32m32n32k32A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m32n32k32m32n32k32A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m32n32k32m32n32k32A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m32n32k32m32n32k32A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m32n32k32m32n32k32A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m32n32k32m32n32k32A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m32n32k32m32n32k32A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m32n32k32m32n32k32A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {16, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m32n32k32m16n32k32A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m32n32k32m16n32k32A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {16, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m32n32k32m16n32k32A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m32n32k32m16n32k32A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {16, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m32n32k32m16n32k32A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m32n32k32m16n32k32A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {16, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m32n32k32m16n32k32A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m32n32k32m16n32k32A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {16, 16, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m32n32k32m16n16k32A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m32n32k32m16n16k32A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {16, 16, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m32n32k32m16n16k32A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m32n32k32m16n16k32A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {16, 16, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m32n32k32m16n16k32A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m32n32k32m16n16k32A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {16, 16, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m32n32k32m16n16k32A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m32n32k32m16n16k32A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {32, 16, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m32n32k32m32n16k32A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m32n32k32m32n16k32A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {32, 16, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m32n32k32m32n16k32A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m32n32k32m32n16k32A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {32, 16, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m32n32k32m32n16k32A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m32n32k32m32n16k32A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {32, 16, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m32n32k32m32n16k32A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m32n32k32m32n16k32A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 64};
  desp.warp_tile_shape = {32, 32, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m64n64k64m32n32k64A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m64n64k64m32n32k64A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 64};
  desp.warp_tile_shape = {32, 32, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m64n64k64m32n32k64A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m64n64k64m32n32k64A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 64};
  desp.warp_tile_shape = {32, 32, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m64n64k64m32n32k64A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m64n64k64m32n32k64A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 64};
  desp.warp_tile_shape = {32, 32, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m64n64k64m32n32k64A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m64n64k64m32n32k64A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 64};
  desp.warp_tile_shape = {32, 64, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m64n128k64m32n64k64A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m64n128k64m32n64k64A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 64};
  desp.warp_tile_shape = {32, 64, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m64n128k64m32n64k64A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m64n128k64m32n64k64A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 64};
  desp.warp_tile_shape = {32, 64, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m64n128k64m32n64k64A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m64n128k64m32n64k64A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 64};
  desp.warp_tile_shape = {32, 64, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m64n128k64m32n64k64A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m64n128k64m32n64k64A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m64n128k32m32n64k32A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m64n128k32m32n64k32A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m64n128k32m32n64k32A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m64n128k32m32n64k32A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m64n128k32m32n64k32A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m64n128k32m32n64k32A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m64n128k32m32n64k32A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m64n128k32m32n64k32A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 64};
  desp.warp_tile_shape = {64, 32, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m128n64k64m64n32k64A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m128n64k64m64n32k64A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 64};
  desp.warp_tile_shape = {64, 32, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m128n64k64m64n32k64A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m128n64k64m64n32k64A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 64};
  desp.warp_tile_shape = {64, 32, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m128n64k64m64n32k64A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m128n64k64m64n32k64A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 64};
  desp.warp_tile_shape = {64, 32, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m128n64k64m64n32k64A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m128n64k64m64n32k64A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 32};
  desp.warp_tile_shape = {64, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m128n64k32m64n32k32A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m128n64k32m64n32k32A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 32};
  desp.warp_tile_shape = {64, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m128n64k32m64n32k32A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m128n64k32m64n32k32A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 32};
  desp.warp_tile_shape = {64, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m128n64k32m64n32k32A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m128n64k32m64n32k32A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 32};
  desp.warp_tile_shape = {64, 32, 32};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m128n64k32m64n32k32A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m128n64k32m64n32k32A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 256, 64};
  desp.warp_tile_shape = {64, 128, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m128n256k64m64n128k64A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m128n256k64m64n128k64A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 256, 64};
  desp.warp_tile_shape = {64, 128, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m128n256k64m64n128k64A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m128n256k64m64n128k64A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 256, 64};
  desp.warp_tile_shape = {64, 128, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m128n256k64m64n128k64A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m128n256k64m64n128k64A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 256, 64};
  desp.warp_tile_shape = {64, 128, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m128n256k64m64n128k64A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m128n256k64m64n128k64A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {256, 128, 64};
  desp.warp_tile_shape = {128, 64, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m256n128k64m128n64k64A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m256n128k64m128n64k64A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {256, 128, 64};
  desp.warp_tile_shape = {128, 64, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m256n128k64m128n64k64A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m256n128k64m128n64k64A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {256, 128, 64};
  desp.warp_tile_shape = {128, 64, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m256n128k64m128n64k64A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m256n128k64m128n64k64A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {256, 128, 64};
  desp.warp_tile_shape = {128, 64, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m256n128k64m128n64k64A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m256n128k64m128n64k64A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 64};
  desp.warp_tile_shape = {64, 64, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m128n128k64m64n64k64A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m128n128k64m64n64k64A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 64};
  desp.warp_tile_shape = {64, 64, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f32tnt_m128n128k64m64n64k64A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f32tnt_m128n128k64m64n64k64A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 64};
  desp.warp_tile_shape = {64, 64, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m128n128k64m64n64k64A1T8816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m128n128k64m64n64k64A1T8816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 64};
  desp.warp_tile_shape = {64, 64, 64};
  desp.tensorop = {8, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Turing";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(7, 5);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Turing_s8s8s8s32f16tnt_m128n128k64m64n64k64A1T8816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Turing_s8s8s8s32f16tnt_m128n128k64m64n64k64A1T8816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 0;
  desp.dtype_b = 0;
  desp.dtype_c = 0;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 16};
  desp.warp_tile_shape = {16, 16, 16};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 1;
  desp.element_per_access_b = 1;
  desp.element_per_access_c = 1;
  desp.access_per_vector = 0;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f32f32f32f32f32tnt_m32n32k16m16n16k16A0T1688_200_C301LLL_SK", "error", desp.__repr__(), "Ampere_f32f32f32f32f32tnt_m32n32k16m16n16k16A0T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 0;
  desp.dtype_b = 0;
  desp.dtype_c = 0;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 16};
  desp.warp_tile_shape = {16, 16, 16};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 4;
  desp.element_per_access_b = 4;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f32f32f32f32f32tnt_m32n32k16m16n16k16A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Ampere_f32f32f32f32f32tnt_m32n32k16m16n16k16A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 0;
  desp.dtype_b = 0;
  desp.dtype_c = 0;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 16};
  desp.warp_tile_shape = {16, 16, 16};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 4;
  desp.element_per_access_b = 4;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f32f32f32f32f32tnt_m32n32k16m16n16k16A1T1688_300_C301LLL_SK", "error", desp.__repr__(), "Ampere_f32f32f32f32f32tnt_m32n32k16m16n16k16A1T1688_300_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 0;
  desp.dtype_b = 0;
  desp.dtype_c = 0;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 16};
  desp.warp_tile_shape = {16, 16, 16};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 4;
  desp.element_per_access_b = 4;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f32f32f32f32f32tnt_m32n32k16m16n16k16A1T1688_400_C301LLL_SK", "error", desp.__repr__(), "Ampere_f32f32f32f32f32tnt_m32n32k16m16n16k16A1T1688_400_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 0;
  desp.dtype_b = 0;
  desp.dtype_c = 0;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 4;
  desp.element_per_access_b = 4;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f32f32f32f32f32tnt_m64n64k32m32n32k32A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Ampere_f32f32f32f32f32tnt_m64n64k32m32n32k32A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 0;
  desp.dtype_b = 0;
  desp.dtype_c = 0;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 4;
  desp.element_per_access_b = 4;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f32f32f32f32f32tnt_m64n64k32m32n32k32A1T1688_300_C301LLL_SK", "error", desp.__repr__(), "Ampere_f32f32f32f32f32tnt_m64n64k32m32n32k32A1T1688_300_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 0;
  desp.dtype_b = 0;
  desp.dtype_c = 0;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 4;
  desp.element_per_access_b = 4;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f32f32f32f32f32tnt_m64n64k32m32n32k32A1T1688_400_C301LLL_SK", "error", desp.__repr__(), "Ampere_f32f32f32f32f32tnt_m64n64k32m32n32k32A1T1688_400_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f16f16f16f16f16tnt_m64n128k32m32n64k32A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Ampere_f16f16f16f16f16tnt_m64n128k32m32n64k32A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f16f16f16f16f16tnt_m64n128k32m32n64k32A1T1688_300_C301LLL_SK", "error", desp.__repr__(), "Ampere_f16f16f16f16f16tnt_m64n128k32m32n64k32A1T1688_300_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f16f16f16f16f16tnt_m64n128k32m32n64k32A1T1688_400_C301LLL_SK", "error", desp.__repr__(), "Ampere_f16f16f16f16f16tnt_m64n128k32m32n64k32A1T1688_400_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f16f16f16f32f32tnt_m64n128k32m32n64k32A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Ampere_f16f16f16f32f32tnt_m64n128k32m32n64k32A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f16f16f16f32f32tnt_m64n128k32m32n64k32A1T1688_300_C301LLL_SK", "error", desp.__repr__(), "Ampere_f16f16f16f32f32tnt_m64n128k32m32n64k32A1T1688_300_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f16f16f16f32f32tnt_m64n128k32m32n64k32A1T1688_400_C301LLL_SK", "error", desp.__repr__(), "Ampere_f16f16f16f32f32tnt_m64n128k32m32n64k32A1T1688_400_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f16f16f16f16f16tnt_m64n64k32m32n32k32A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Ampere_f16f16f16f16f16tnt_m64n64k32m32n32k32A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f16f16f16f16f16tnt_m64n64k32m32n32k32A1T1688_300_C301LLL_SK", "error", desp.__repr__(), "Ampere_f16f16f16f16f16tnt_m64n64k32m32n32k32A1T1688_300_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f16f16f16f16f16tnt_m64n64k32m32n32k32A1T1688_400_C301LLL_SK", "error", desp.__repr__(), "Ampere_f16f16f16f16f16tnt_m64n64k32m32n32k32A1T1688_400_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f16f16f16f32f32tnt_m64n64k32m32n32k32A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Ampere_f16f16f16f32f32tnt_m64n64k32m32n32k32A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f16f16f16f32f32tnt_m64n64k32m32n32k32A1T1688_300_C301LLL_SK", "error", desp.__repr__(), "Ampere_f16f16f16f32f32tnt_m64n64k32m32n32k32A1T1688_300_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f16f16f16f32f32tnt_m64n64k32m32n32k32A1T1688_400_C301LLL_SK", "error", desp.__repr__(), "Ampere_f16f16f16f32f32tnt_m64n64k32m32n32k32A1T1688_400_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 32};
  desp.warp_tile_shape = {64, 32, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f16f16f16f16f16tnt_m128n64k32m64n32k32A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Ampere_f16f16f16f16f16tnt_m128n64k32m64n32k32A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 32};
  desp.warp_tile_shape = {64, 32, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f16f16f16f16f16tnt_m128n64k32m64n32k32A1T1688_300_C301LLL_SK", "error", desp.__repr__(), "Ampere_f16f16f16f16f16tnt_m128n64k32m64n32k32A1T1688_300_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 7;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 32};
  desp.warp_tile_shape = {64, 32, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f16f16f16f16f16tnt_m128n64k32m64n32k32A1T1688_400_C301LLL_SK", "error", desp.__repr__(), "Ampere_f16f16f16f16f16tnt_m128n64k32m64n32k32A1T1688_400_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 32};
  desp.warp_tile_shape = {64, 32, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f16f16f16f32f32tnt_m128n64k32m64n32k32A1T1688_200_C301LLL_SK", "error", desp.__repr__(), "Ampere_f16f16f16f32f32tnt_m128n64k32m64n32k32A1T1688_200_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 32};
  desp.warp_tile_shape = {64, 32, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f16f16f16f32f32tnt_m128n64k32m64n32k32A1T1688_300_C301LLL_SK", "error", desp.__repr__(), "Ampere_f16f16f16f32f32tnt_m128n64k32m64n32k32A1T1688_300_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 7;
  desp.dtype_b = 7;
  desp.dtype_c = 7;
  desp.dacc = 0;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 32};
  desp.warp_tile_shape = {64, 32, 32};
  desp.tensorop = {16, 8, 8};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 8;
  desp.element_per_access_b = 8;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = false;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_f16f16f16f32f32tnt_m128n64k32m64n32k32A1T1688_400_C301LLL_SK", "error", desp.__repr__(), "Ampere_f16f16f16f32f32tnt_m128n64k32m64n32k32A1T1688_400_C301LLL_SK");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 0;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f32s32f32tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8f32s32f32tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 0;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f32s32f32tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8f32s32f32tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 0;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f32s32f32tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8f32s32f32tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 0;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f32s32f32tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8f32s32f32tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 0;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f32s32f32tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8f32s32f32tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 0;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f32s32f32tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8f32s32f32tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 0;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f32s32f16tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8f32s32f16tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 0;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f32s32f16tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8f32s32f16tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 0;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f32s32f16tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8f32s32f16tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 0;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f32s32f16tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8f32s32f16tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 0;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f32s32f16tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8f32s32f16tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 0;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 4;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f32s32f16tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8f32s32f16tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 7;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f16s32f32tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8f16s32f32tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 7;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f16s32f32tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8f16s32f32tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 7;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f16s32f32tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8f16s32f32tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 7;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f16s32f32tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8f16s32f32tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 7;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f16s32f32tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8f16s32f32tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 7;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f16s32f32tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8f16s32f32tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 7;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f16s32f16tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8f16s32f16tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 7;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f16s32f16tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8f16s32f16tnt_m64n64k32m32n32k32A1T16816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 7;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f16s32f16tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8f16s32f16tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 7;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f16s32f16tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8f16s32f16tnt_m64n64k32m32n32k32A1T16816_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 7;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f16s32f16tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8f16s32f16tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 7;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8f16s32f16tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8f16s32f16tnt_m64n64k32m32n32k32A1T16816_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m32n32k32m32n32k32A1T16816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m32n32k32m32n32k32A1T16816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m32n32k32m32n32k32A1T16816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m32n32k32m32n32k32A1T16816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m32n32k32m32n32k32A1T16816_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m32n32k32m32n32k32A1T16816_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m32n32k32m32n32k32A1T16816_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m32n32k32m32n32k32A1T16816_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m32n32k32m32n32k32A1T16816_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m32n32k32m32n32k32A1T16816_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m32n32k32m32n32k32A1T16816_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m32n32k32m32n32k32A1T16816_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m32n32k32m32n32k32A1T16816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m32n32k32m32n32k32A1T16816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m32n32k32m32n32k32A1T16816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m32n32k32m32n32k32A1T16816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m32n32k32m32n32k32A1T16816_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m32n32k32m32n32k32A1T16816_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m32n32k32m32n32k32A1T16816_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m32n32k32m32n32k32A1T16816_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m32n32k32m32n32k32A1T16816_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m32n32k32m32n32k32A1T16816_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m32n32k32m32n32k32A1T16816_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m32n32k32m32n32k32A1T16816_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n32k32m32n32k32A1T16816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n32k32m32n32k32A1T16816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n32k32m32n32k32A1T16816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n32k32m32n32k32A1T16816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n32k32m32n32k32A1T16816_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n32k32m32n32k32A1T16816_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n32k32m32n32k32A1T16816_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n32k32m32n32k32A1T16816_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n32k32m32n32k32A1T16816_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n32k32m32n32k32A1T16816_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n32k32m32n32k32A1T16816_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n32k32m32n32k32A1T16816_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n32k32m32n32k32A1T16816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n32k32m32n32k32A1T16816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n32k32m32n32k32A1T16816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n32k32m32n32k32A1T16816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n32k32m32n32k32A1T16816_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n32k32m32n32k32A1T16816_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n32k32m32n32k32A1T16816_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n32k32m32n32k32A1T16816_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n32k32m32n32k32A1T16816_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n32k32m32n32k32A1T16816_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 32, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n32k32m32n32k32A1T16816_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n32k32m32n32k32A1T16816_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m32n64k32m32n32k32A1T16816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m32n64k32m32n32k32A1T16816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m32n64k32m32n32k32A1T16816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m32n64k32m32n32k32A1T16816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m32n64k32m32n32k32A1T16816_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m32n64k32m32n32k32A1T16816_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m32n64k32m32n32k32A1T16816_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m32n64k32m32n32k32A1T16816_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m32n64k32m32n32k32A1T16816_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m32n64k32m32n32k32A1T16816_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m32n64k32m32n32k32A1T16816_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m32n64k32m32n32k32A1T16816_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m32n64k32m32n32k32A1T16816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m32n64k32m32n32k32A1T16816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m32n64k32m32n32k32A1T16816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m32n64k32m32n32k32A1T16816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m32n64k32m32n32k32A1T16816_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m32n64k32m32n32k32A1T16816_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m32n64k32m32n32k32A1T16816_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m32n64k32m32n32k32A1T16816_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m32n64k32m32n32k32A1T16816_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m32n64k32m32n32k32A1T16816_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {32, 64, 32};
  desp.warp_tile_shape = {32, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m32n64k32m32n32k32A1T16816_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m32n64k32m32n32k32A1T16816_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 64};
  desp.warp_tile_shape = {64, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n64k64m64n32k64A1T16832_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n64k64m64n32k64A1T16832_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 64};
  desp.warp_tile_shape = {64, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n64k64m64n32k64A1T16832_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n64k64m64n32k64A1T16832_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 64};
  desp.warp_tile_shape = {64, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n64k64m64n32k64A1T16832_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n64k64m64n32k64A1T16832_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 64};
  desp.warp_tile_shape = {64, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n64k64m64n32k64A1T16832_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n64k64m64n32k64A1T16832_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 64};
  desp.warp_tile_shape = {64, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n64k64m64n32k64A1T16832_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n64k64m64n32k64A1T16832_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 64};
  desp.warp_tile_shape = {64, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n64k64m64n32k64A1T16832_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n64k64m64n32k64A1T16832_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 64};
  desp.warp_tile_shape = {64, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n64k64m64n32k64A1T16832_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n64k64m64n32k64A1T16832_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 64};
  desp.warp_tile_shape = {64, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n64k64m64n32k64A1T16832_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n64k64m64n32k64A1T16832_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 64};
  desp.warp_tile_shape = {64, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n64k64m64n32k64A1T16832_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n64k64m64n32k64A1T16832_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 64};
  desp.warp_tile_shape = {64, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n64k64m64n32k64A1T16832_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n64k64m64n32k64A1T16832_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 64};
  desp.warp_tile_shape = {64, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n64k64m64n32k64A1T16832_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n64k64m64n32k64A1T16832_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 64};
  desp.warp_tile_shape = {64, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n64k64m64n32k64A1T16832_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n64k64m64n32k64A1T16832_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 32};
  desp.warp_tile_shape = {64, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n64k32m64n32k32A1T16816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n64k32m64n32k32A1T16816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 32};
  desp.warp_tile_shape = {64, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n64k32m64n32k32A1T16816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n64k32m64n32k32A1T16816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 32};
  desp.warp_tile_shape = {64, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n64k32m64n32k32A1T16816_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n64k32m64n32k32A1T16816_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 32};
  desp.warp_tile_shape = {64, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n64k32m64n32k32A1T16816_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n64k32m64n32k32A1T16816_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 32};
  desp.warp_tile_shape = {64, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n64k32m64n32k32A1T16816_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n64k32m64n32k32A1T16816_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 32};
  desp.warp_tile_shape = {64, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n64k32m64n32k32A1T16816_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n64k32m64n32k32A1T16816_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 32};
  desp.warp_tile_shape = {64, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n64k32m64n32k32A1T16816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n64k32m64n32k32A1T16816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 32};
  desp.warp_tile_shape = {64, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n64k32m64n32k32A1T16816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n64k32m64n32k32A1T16816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 32};
  desp.warp_tile_shape = {64, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n64k32m64n32k32A1T16816_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n64k32m64n32k32A1T16816_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 32};
  desp.warp_tile_shape = {64, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n64k32m64n32k32A1T16816_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n64k32m64n32k32A1T16816_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 32};
  desp.warp_tile_shape = {64, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n64k32m64n32k32A1T16816_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n64k32m64n32k32A1T16816_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 64, 32};
  desp.warp_tile_shape = {64, 32, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n64k32m64n32k32A1T16816_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n64k32m64n32k32A1T16816_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 64};
  desp.warp_tile_shape = {32, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n128k64m32n64k64A1T16832_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n128k64m32n64k64A1T16832_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 64};
  desp.warp_tile_shape = {32, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n128k64m32n64k64A1T16832_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n128k64m32n64k64A1T16832_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 64};
  desp.warp_tile_shape = {32, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n128k64m32n64k64A1T16832_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n128k64m32n64k64A1T16832_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 64};
  desp.warp_tile_shape = {32, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n128k64m32n64k64A1T16832_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n128k64m32n64k64A1T16832_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 64};
  desp.warp_tile_shape = {32, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n128k64m32n64k64A1T16832_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n128k64m32n64k64A1T16832_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 64};
  desp.warp_tile_shape = {32, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n128k64m32n64k64A1T16832_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n128k64m32n64k64A1T16832_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 64};
  desp.warp_tile_shape = {32, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n128k64m32n64k64A1T16832_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n128k64m32n64k64A1T16832_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 64};
  desp.warp_tile_shape = {32, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n128k64m32n64k64A1T16832_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n128k64m32n64k64A1T16832_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 64};
  desp.warp_tile_shape = {32, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n128k64m32n64k64A1T16832_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n128k64m32n64k64A1T16832_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 64};
  desp.warp_tile_shape = {32, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n128k64m32n64k64A1T16832_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n128k64m32n64k64A1T16832_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 64};
  desp.warp_tile_shape = {32, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n128k64m32n64k64A1T16832_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n128k64m32n64k64A1T16832_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 64};
  desp.warp_tile_shape = {32, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n128k64m32n64k64A1T16832_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n128k64m32n64k64A1T16832_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n128k32m32n64k32A1T16816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n128k32m32n64k32A1T16816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n128k32m32n64k32A1T16816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n128k32m32n64k32A1T16816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n128k32m32n64k32A1T16816_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n128k32m32n64k32A1T16816_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n128k32m32n64k32A1T16816_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n128k32m32n64k32A1T16816_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n128k32m32n64k32A1T16816_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n128k32m32n64k32A1T16816_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n128k32m32n64k32A1T16816_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n128k32m32n64k32A1T16816_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n128k32m32n64k32A1T16816_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n128k32m32n64k32A1T16816_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n128k32m32n64k32A1T16816_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n128k32m32n64k32A1T16816_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n128k32m32n64k32A1T16816_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n128k32m32n64k32A1T16816_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n128k32m32n64k32A1T16816_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n128k32m32n64k32A1T16816_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n128k32m32n64k32A1T16816_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n128k32m32n64k32A1T16816_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 128, 32};
  desp.warp_tile_shape = {32, 64, 32};
  desp.tensorop = {16, 8, 16};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n128k32m32n64k32A1T16816_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n128k32m32n64k32A1T16816_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 64};
  desp.warp_tile_shape = {32, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n64k64m32n32k64A1T16832_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n64k64m32n32k64A1T16832_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 64};
  desp.warp_tile_shape = {32, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n64k64m32n32k64A1T16832_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n64k64m32n32k64A1T16832_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 64};
  desp.warp_tile_shape = {32, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n64k64m32n32k64A1T16832_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n64k64m32n32k64A1T16832_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 64};
  desp.warp_tile_shape = {32, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n64k64m32n32k64A1T16832_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n64k64m32n32k64A1T16832_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 64};
  desp.warp_tile_shape = {32, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n64k64m32n32k64A1T16832_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n64k64m32n32k64A1T16832_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 64};
  desp.warp_tile_shape = {32, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m64n64k64m32n32k64A1T16832_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m64n64k64m32n32k64A1T16832_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 64};
  desp.warp_tile_shape = {32, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n64k64m32n32k64A1T16832_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n64k64m32n32k64A1T16832_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 64};
  desp.warp_tile_shape = {32, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n64k64m32n32k64A1T16832_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n64k64m32n32k64A1T16832_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 64};
  desp.warp_tile_shape = {32, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n64k64m32n32k64A1T16832_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n64k64m32n32k64A1T16832_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 64};
  desp.warp_tile_shape = {32, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n64k64m32n32k64A1T16832_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n64k64m32n32k64A1T16832_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 64};
  desp.warp_tile_shape = {32, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n64k64m32n32k64A1T16832_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n64k64m32n32k64A1T16832_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {64, 64, 64};
  desp.warp_tile_shape = {32, 32, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m64n64k64m32n32k64A1T16832_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m64n64k64m32n32k64A1T16832_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 64};
  desp.warp_tile_shape = {64, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n128k64m64n64k64A1T16832_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n128k64m64n64k64A1T16832_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 64};
  desp.warp_tile_shape = {64, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n128k64m64n64k64A1T16832_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n128k64m64n64k64A1T16832_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 64};
  desp.warp_tile_shape = {64, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n128k64m64n64k64A1T16832_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n128k64m64n64k64A1T16832_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 64};
  desp.warp_tile_shape = {64, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n128k64m64n64k64A1T16832_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n128k64m64n64k64A1T16832_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 64};
  desp.warp_tile_shape = {64, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n128k64m64n64k64A1T16832_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n128k64m64n64k64A1T16832_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 64};
  desp.warp_tile_shape = {64, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n128k64m64n64k64A1T16832_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n128k64m64n64k64A1T16832_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 64};
  desp.warp_tile_shape = {64, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n128k64m64n64k64A1T16832_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n128k64m64n64k64A1T16832_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 64};
  desp.warp_tile_shape = {64, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n128k64m64n64k64A1T16832_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n128k64m64n64k64A1T16832_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 64};
  desp.warp_tile_shape = {64, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n128k64m64n64k64A1T16832_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n128k64m64n64k64A1T16832_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 64};
  desp.warp_tile_shape = {64, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n128k64m64n64k64A1T16832_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n128k64m64n64k64A1T16832_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 64};
  desp.warp_tile_shape = {64, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n128k64m64n64k64A1T16832_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n128k64m64n64k64A1T16832_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 64};
  desp.warp_tile_shape = {64, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n128k64m64n64k64A1T16832_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n128k64m64n64k64A1T16832_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 256, 64};
  desp.warp_tile_shape = {64, 128, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n256k64m64n128k64A1T16832_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n256k64m64n128k64A1T16832_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 256, 64};
  desp.warp_tile_shape = {64, 128, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n256k64m64n128k64A1T16832_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n256k64m64n128k64A1T16832_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 256, 64};
  desp.warp_tile_shape = {64, 128, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n256k64m64n128k64A1T16832_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n256k64m64n128k64A1T16832_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 256, 64};
  desp.warp_tile_shape = {64, 128, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n256k64m64n128k64A1T16832_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n256k64m64n128k64A1T16832_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 256, 64};
  desp.warp_tile_shape = {64, 128, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n256k64m64n128k64A1T16832_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n256k64m64n128k64A1T16832_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 256, 64};
  desp.warp_tile_shape = {64, 128, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n256k64m64n128k64A1T16832_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n256k64m64n128k64A1T16832_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 256, 64};
  desp.warp_tile_shape = {64, 128, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n256k64m64n128k64A1T16832_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n256k64m64n128k64A1T16832_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 256, 64};
  desp.warp_tile_shape = {64, 128, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n256k64m64n128k64A1T16832_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n256k64m64n128k64A1T16832_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 256, 64};
  desp.warp_tile_shape = {64, 128, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n256k64m64n128k64A1T16832_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n256k64m64n128k64A1T16832_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 256, 64};
  desp.warp_tile_shape = {64, 128, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n256k64m64n128k64A1T16832_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n256k64m64n128k64A1T16832_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 256, 64};
  desp.warp_tile_shape = {64, 128, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n256k64m64n128k64A1T16832_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n256k64m64n128k64A1T16832_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 256, 64};
  desp.warp_tile_shape = {64, 128, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n256k64m64n128k64A1T16832_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n256k64m64n128k64A1T16832_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {256, 128, 64};
  desp.warp_tile_shape = {128, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m256n128k64m128n64k64A1T16832_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m256n128k64m128n64k64A1T16832_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {256, 128, 64};
  desp.warp_tile_shape = {128, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m256n128k64m128n64k64A1T16832_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m256n128k64m128n64k64A1T16832_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {256, 128, 64};
  desp.warp_tile_shape = {128, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m256n128k64m128n64k64A1T16832_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m256n128k64m128n64k64A1T16832_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {256, 128, 64};
  desp.warp_tile_shape = {128, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m256n128k64m128n64k64A1T16832_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m256n128k64m128n64k64A1T16832_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {256, 128, 64};
  desp.warp_tile_shape = {128, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m256n128k64m128n64k64A1T16832_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m256n128k64m128n64k64A1T16832_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {256, 128, 64};
  desp.warp_tile_shape = {128, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m256n128k64m128n64k64A1T16832_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m256n128k64m128n64k64A1T16832_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {256, 128, 64};
  desp.warp_tile_shape = {128, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m256n128k64m128n64k64A1T16832_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m256n128k64m128n64k64A1T16832_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {256, 128, 64};
  desp.warp_tile_shape = {128, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m256n128k64m128n64k64A1T16832_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m256n128k64m128n64k64A1T16832_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {256, 128, 64};
  desp.warp_tile_shape = {128, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m256n128k64m128n64k64A1T16832_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m256n128k64m128n64k64A1T16832_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {256, 128, 64};
  desp.warp_tile_shape = {128, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m256n128k64m128n64k64A1T16832_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m256n128k64m128n64k64A1T16832_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {256, 128, 64};
  desp.warp_tile_shape = {128, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m256n128k64m128n64k64A1T16832_400_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m256n128k64m128n64k64A1T16832_400_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {256, 128, 64};
  desp.warp_tile_shape = {128, 64, 64};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 4;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 16;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m256n128k64m128n64k64A1T16832_400_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m256n128k64m128n64k64A1T16832_400_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 128};
  desp.warp_tile_shape = {64, 64, 128};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n128k128m64n64k128A1T16832_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n128k128m64n64k128A1T16832_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 128};
  desp.warp_tile_shape = {64, 64, 128};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n128k128m64n64k128A1T16832_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n128k128m64n64k128A1T16832_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 128};
  desp.warp_tile_shape = {64, 64, 128};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n128k128m64n64k128A1T16832_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n128k128m64n64k128A1T16832_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 0;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 128};
  desp.warp_tile_shape = {64, 64, 128};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f32tnt_m128n128k128m64n64k128A1T16832_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f32tnt_m128n128k128m64n64k128A1T16832_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 128};
  desp.warp_tile_shape = {64, 64, 128};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n128k128m64n64k128A1T16832_200_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n128k128m64n64k128A1T16832_200_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 128};
  desp.warp_tile_shape = {64, 64, 128};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 2;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n128k128m64n64k128A1T16832_200_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n128k128m64n64k128A1T16832_200_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 128};
  desp.warp_tile_shape = {64, 64, 128};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = false;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n128k128m64n64k128A1T16832_300_C301LLL_SK_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n128k128m64n64k128A1T16832_300_C301LLL_SK_S8");
  desps.push_back(desp);
  }
  {
  tv::gemm::ConvAlgoDesp desp(3, tv::gemm::ConvOpType(0));
  desp.dtype_a = 3;
  desp.dtype_b = 3;
  desp.dtype_c = 3;
  desp.dacc = 1;
  desp.dcomp = 7;
  desp.trans_a_set(false);
  desp.trans_b_set(true);
  desp.trans_c_set(false);
  desp.tile_shape = {128, 128, 128};
  desp.warp_tile_shape = {64, 64, 128};
  desp.tensorop = {16, 8, 32};
  desp.num_stage = 3;
  desp.algo = "Ampere";
  desp.split_k_serial_set(false);
  desp.split_k_parallel_set(false);
  desp.shuffle_type = static_cast<tv::gemm::ShuffleStrideType>(0);
  desp.element_per_access_a = 16;
  desp.element_per_access_b = 16;
  desp.element_per_access_c = 8;
  desp.access_per_vector = 1;
  desp.min_arch = std::make_tuple(8, 0);
  // Conv attrs
  desp.ndim = 3;
  desp.op_type = static_cast<tv::gemm::ConvOpType>(0);
  desp.iter_algo = static_cast<tv::gemm::ConvIterAlgo>(1);
  desp.layout_i = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_w = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.layout_o = static_cast<tv::gemm::ConvLayoutType>(1);
  desp.interleave_i = 1;
  desp.interleave_w = 1;
  desp.interleave_o = 1;
  desp.mask_sparse = true;
  desp.increment_k_first = true;
  desp.is_int8_inference = true;
  desp.dynamic_mask = true;
  TV_ASSERT_RT_ERR(desp.__repr__() == "Ampere_s8s8s8s32f16tnt_m128n128k128m64n64k128A1T16832_300_C301LLL_SKD_S8", "error", desp.__repr__(), "Ampere_s8s8s8s32f16tnt_m128n128k128m64n64k128A1T16832_300_C301LLL_SKD_S8");
  desps.push_back(desp);
  }
  return desps;
}
} // namespace main
} // namespace conv
} // namespace cumm
} // namespace spconvlib